[{"title":"About me","date":"","description":"Who I am","body":" Hello, I don\u0026rsquo;t like to talk about myself. Nevertheless, every visitor should get the possibility to get an approximate picture about me, my experiences and my know-how, so that maybe my choice of topics, opinions and other statements can be better understood.\nI was born in the 1980s as part of a large family and grew up in a small town in East Westphalia (Germany). Besides the disadvantages of having older siblings, it also had advantages. One of them was that I already had access to a Commodore C64 in kindergarten and was thus able to discover my passion for computers early on, especially for that time, through video games.\nDuring my school time I started programming besides gaming. My first steps were HTML, CSS, Logo and QBasic. In high school PHP, C and SQL followed. But I didn\u0026rsquo;t spend all my time at school in front of the PC, I also made friends that last until today, even if we live hundreds of kilometers apart and therefore rarely see each other. In addition, I also met my now wife during this time. I spent a total of 15 years in various types of schools before starting the \u0026ldquo;serious\u0026rdquo; part of my life as an IT assistant with a technical college entrance qualification. After that, the interplay between (further) education and \u0026ldquo;work\u0026rdquo; began for me:\n Dual apprenticeship as an IT specialist, specialization: application development (2,5 years) Consulting across Germany, mostly in the Unix-/Linux-Operations sector (~2 years) Bachelor of Science in Computer science (3,5 years) Software Developer, mostly as a backend developer (~3.5 years) Master of Science in Computer Science (1.5 years) Brief excursion into research (2.5 months) Software Developer  There were other stations and also during the study I worked partly, but I hope that my meandering can be recognized. Something that becomes a problem for me every now and then is that I would prefer to acquire deeper knowledge about all areas of computer science. As a result, it happens from time to time that I go down the rabbit hole when doing research and it takes me a long time to resurface. My professional focus has remained in software development over the years, but since I have been using Linux almost exclusively privately (also for gaming) since about 2010, my knowledge in this area is also evolving. When it comes to programming languages, I\u0026rsquo;ve noticed over the years that the compiler is my friend and therefore I\u0026rsquo;ve become a big proponent of statically typed languages.\nNow an attempt to describe myself in bullet points:\n Languages I like:  SQL Kotlin Elm Haskell Rust C# Java   Operating systems I like:  GNU/Linux, currently especially in the form of Arch Linux and EndeavourOS All UNIXes, where root is really root (I mean you, MacOS)   Software I like:  Open Source Software ðŸ˜‰   Video games I like:  California Games Sid Meier\u0026rsquo;s Pirates! Mafia (C64), even if I got know it as Al Capone Jr. Die 4te Offenbarung, when the old community still exists (before 2005) Ultima Online (misc. FreeShards) World of Warcraft (only up to and including TBC) Quake series, in particular Quake 3 Arena (including CPMA) Doom series, especially Doom (2016) Monkey-Island series Point\u0026amp;Click adventures by King Art, first and foremost The Book of Unwritten Tales Diablo series, especially Diablo II including Lord of Destruction Hellblade: Senuaâ€™s Sacrifice Hunt: Showdown   Films I like:  Hackers Matrix film series Inception The Illusionist The Prestige Equilibrium Ocean\u0026rsquo;s film series Shutter Island Pan\u0026rsquo;s Labyrinth   Authors I like:  Markus Heitz Richard Schwartz Joe Abercrombie    ","ref":"/about/"},{"title":"Privacy Policy","date":"","description":"","body":"Personal data (usually referred to just as \u0026ldquo;data\u0026rdquo; below) will only be processed by us to the extent necessary and for the purpose of providing a functional and user-friendly website, including its contents, and the services offered there.\nPer Art. 4 No. 1 of Regulation (EU) 2016/679, i.e. the General Data Protection Regulation (hereinafter referred to as the \u0026ldquo;GDPR\u0026rdquo;), \u0026ldquo;processing\u0026rdquo; refers to any operation or set of operations such as collection, recording, organization, structuring, storage, adaptation, alteration, retrieval, consultation, use, disclosure by transmission, dissemination, or otherwise making available, alignment, or combination, restriction, erasure, or destruction performed on personal data, whether by automated means or not.\nThe following privacy policy is intended to inform you in particular about the type, scope, purpose, duration, and legal basis for the processing of such data either under our own control or in conjunction with others. We also inform you below about the third-party components we use to optimize our website and improve the user experience which may result in said third parties also processing data they collect and control.\nOur privacy policy is structured as follows:\nI. Information about us as controllers of your data\nII. The rights of users and data subjects\nIII. Information about the data processing\nI. Information about us as controllers of your data The party responsible for this website (the \u0026ldquo;controller\u0026rdquo;) for purposes of data protection law is:\nTobias Wink\nErasmusweg 8\n75015 Bretten\nDeutschland\nE-Mail: info@sainth.de\nII. The rights of users and data subjects With regard to the data processing to be described in more detail below, users and data subjects have the right\n to confirmation of whether data concerning them is being processed, information about the data being processed, further information about the nature of the data processing, and copies of the data (cf. also Art. 15 GDPR); to correct or complete incorrect or incomplete data (cf. also Art. 16 GDPR); to the immediate deletion of data concerning them (cf. also Art. 17 DSGVO), or, alternatively, if further processing is necessary as stipulated in Art. 17 Para. 3 GDPR, to restrict said processing per Art. 18 GDPR; to receive copies of the data concerning them and/or provided by them and to have the same transmitted to other providers/controllers (cf. also Art. 20 GDPR); to file complaints with the supervisory authority if they believe that data concerning them is being processed by the controller in breach of data protection provisions (see also Art. 77 GDPR).  In addition, the controller is obliged to inform all recipients to whom it discloses data of any such corrections, deletions, or restrictions placed on processing the same per Art. 16, 17 Para. 1, 18 GDPR. However, this obligation does not apply if such notification is impossible or involves a disproportionate effort. Nevertheless, users have a right to information about these recipients.\nLikewise, under Art. 21 GDPR, users and data subjects have the right to object to the controller\u0026rsquo;s future processing of their data pursuant to Art. 6 Para. 1 lit. f) GDPR. In particular, an objection to data processing for the purpose of direct advertising is permissible.\nIII. Information about the data processing Your data processed when using our website will be deleted or blocked as soon as the purpose for its storage ceases to apply, provided the deletion of the same is not in breach of any statutory storage obligations or unless otherwise stipulated below.\nServer data For technical reasons, the following data sent by your internet browser to us or to our server provider will be collected, especially to ensure a secure and stable website: These server log files record the type and version of your browser, operating system, the website from which you came (referrer URL), the webpages on our site visited, the date and time of your visit, as well as the IP address from which you visited our site.\nThe data thus collected will be temporarily stored, but not in association with any other of your data.\nThe basis for this storage is Art. 6 Para. 1 lit. f) GDPR. Our legitimate interest lies in the improvement, stability, functionality, and security of our website.\nThe data will be deleted within no more than seven days, unless continued storage is required for evidentiary purposes. In which case, all or part of the data will be excluded from deletion until the investigation of the relevant incident is finally resolved.\nCookies a) Session cookies We use cookies on our website. Cookies are small text files or other storage technologies stored on your computer by your browser. These cookies process certain specific information about you, such as your browser, location data, or IP address.\nThis processing makes our website more user-friendly, efficient, and secure, allowing us, for example, to display our website in different languages or to offer a shopping cart function.\nThe legal basis for such processing is Art. 6 Para. 1 lit. b) GDPR, insofar as these cookies are used to collect data to initiate or process contractual relationships.\nIf the processing does not serve to initiate or process a contract, our legitimate interest lies in improving the functionality of our website. The legal basis is then Art. 6 Para. 1 lit. f) GDPR.\nWhen you close your browser, these session cookies are deleted.\nb) Third-party cookies If necessary, our website may also use cookies from companies with whom we cooperate for the purpose of advertising, analyzing, or improving the features of our website.\nPlease refer to the following information for details, in particular for the legal basis and purpose of such third-party collection and processing of data collected through cookies.\nc) Disabling cookies You can refuse the use of cookies by changing the settings on your browser. Likewise, you can use the browser to delete cookies that have already been stored. However, the steps and measures required vary, depending on the browser you use. If you have any questions, please use the help function or consult the documentation for your browser or contact its maker for support. Browser settings cannot prevent so-called flash cookies from being set. Instead, you will need to change the setting of your Flash player. The steps and measures required for this also depend on the Flash player you are using. If you have any questions, please use the help function or consult the documentation for your Flash player or contact its maker for support.\nIf you prevent or restrict the installation of cookies, not all of the functions on our site may be fully usable.\nContact If you contact us via email or the contact form, the data you provide will be used for the purpose of processing your request. We must have this data in order to process and answer your inquiry; otherwise we will not be able to answer it in full or at all.\nThe legal basis for this data processing is Art. 6 Para. 1 lit. b) GDPR.\nYour data will be deleted once we have fully answered your inquiry and there is no further legal obligation to store your data, such as if an order or contract resulted therefrom.\nUser posts, comments, and ratings We offer you the opportunity to post questions, answers, opinions, and ratings on our website, hereinafter referred to jointly as \u0026ldquo;posts.\u0026rdquo; If you make use of this opportunity, we will process and publish your post, the date and time you submitted it, and any pseudonym you may have used.\nThe legal basis for this is Art. 6 Para. 1 lit. a) GDPR. You may revoke your prior consent under Art. 7 Para. 3 GDPR with future effect. All you have to do is inform us that you are revoking your consent.\nIn addition, we will also process your IP address and email address. The IP address is processed because we might have a legitimate interest in taking or supporting further action if your post infringes the rights of third parties and/or is otherwise unlawful.\nIn this case, the legal basis is Art. 6 Para. 1 lit. f) GDPR. Our legitimate interest lies in any legal defense we may have to mount.\nModel Data Protection Statement for Anwaltskanzlei WeiÃŸ \u0026amp; Partner\n","ref":"/privacy_policy/"},{"title":"Projects","date":"","description":"","body":"","ref":"/projects/"},{"title":"Publications","date":"","description":"","body":" Masterthesis [EN]: A decentralized communication approach for federated learning Term paper [DE]: Attacks on SSL 3.0 and TLS 1.0 using CBC mode and predecessor versions of these attacks Bachelorthesis [DE]: Generation of the persistence layer by creating a domain-specific language  ","ref":"/publications/"},{"title":"JWTs, a supplement to BasicAuth","date":"","description":"How to use JWTs with BasicAuth.","body":"Most REST APIs support BasicAuth when they require authentication and manage user data themselves. When choosing the function to protect stored passwords, it is important to find the right balance between user convenience and attacker protection. On the one hand, you want to keep adversaries at bay for as long as possible, but on the other hand, you also want to give users the shortest possible response times. All adaptive password hashing methods recommended by OWASP therefore offer the option of configuring the speed of the algorithm via a work factor. In order to be able to select this work factor as high as possible, without annoying the users unnecessarily, the use of JWTs lends itself.\nWhat is a JWT? A JSON Web Token, or JWT (pronounced \u0026ldquo;jot\u0026rdquo;), is a simple and compact format for transferring Claims. JWTs are designed to be used in areas that are sensitive to whitespace, such as HTTP Authorization headers. A JWT is really just a JSON object that contains the Claims as content. In the related RFC 7519, some Claims are predefined, in my opinion the most important ones:\n sub-Claim (Subject): Contains the principal for the JWT. exp-Claim (Expiration Date): Defines the expiration date for the JWT. After this timestamp has expired, the JWT may no longer be accepted or processed. jti-Claim (JWT ID): Represents a unique ID for the JWT. With the help of this claim it should be possible to distinguish JWTs despite similar other content and thus prevent replay attacks.  If the predefined Claims are not sufficient, you may add private Claims to them as long as there are no conflicts with the public Claims.\nHow to use them? JWTs are used as payload for JWS elements or also as plaintext within JWE elements, but I will limit myself to JWS elements here. A JWS element consists of the three sections header, payload and signature, each separated by a dot \u0026ldquo;.\u0026rdquo;. The following is an example of a fully encoded JWT - for better readability the sections have been moved to separate lines:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. eyJzdWIiOiIxMjM0NTY3ODkwIiwicm9sZSI6ImFkbWluIn0. OC16ViAXu0_JaKXlUKQVHCKeZn1tXTwcwMDRpmY6xyc Header and payload are \u0026lt;abbr title=\u0026ldquo;Modified version of Base64 where \u0026ldquo;+\u0026rdquo; and \u0026ldquo;/\u0026rdquo; are replaced by \u0026ldquo;-\u0026rdquo; and \u0026ldquo;\u0026rdquo; respectively.\u0026quot;\u0026gt;Base64 URL encoded JSON objects. The signature is created over the header and payload values concatenated with \u0026ldquo;.\u0026rdquo; and already encoded, and is also \u0026lt;abbr title=\u0026ldquo;Modified version of Base64 where \u0026ldquo;+\u0026rdquo; and \u0026ldquo;/\u0026rdquo; are replaced by \u0026ldquo;-\u0026rdquo; and \u0026ldquo;\u0026rdquo; respectively.\u0026quot;\u0026gt;Base64 URL encoded. The algorithm used for the signature is specified in the header claim alg. A possible pseudocode example of signature creation for the previous example would be:\nbase64UrlEncode( HMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), \u0026#34;secret\u0026#34; ) ) According to RFC 7515, JWS implementations must support only one algorithm for signing: HMAC with SHA-256, also called HS256. But it is recommended to support RS256 (RSASSA PKCS1 v1.5 with SHA-256) and ES256 (ECDSA with P-256 and SHA-256) as well. Many libraries also support corresponding variations of the three algorithms with SHA-384 or SHA-512 instead of SHA-256.\nAll signature methods serve the same purpose: they allow to ensure the authenticity of the JWT claims. All HMAC procedures use a shared secret for generation/validation, for this reason the trust between parties using JWS with HMAC procedures must be very high. If the trust between the parties is not so high (e.g. different operators), a procedure based on RSASSA or ECDSA should definitely be used, since both procedures are public-key algorithms. Thus, the JWT is signed by the issuer with the private key when it is generated and anyone who has the associated public key can subsequently validate the signature.\nHow to use JWS in conjunction with BasicAuth? BasicAuth should only be used for the first request. In each response a new token should be sent or stored in the cookie, ideally with a new value for jti claim (which of course should also be evaluated on the server side). The way over the cookie is to be preferred in my opinion, if it is possible, because you don\u0026rsquo;t have to think about reading the token on the frontend side to send it again with the next request. Furthermore it increases the security if the cookie has the HttpOnly flag, because you can\u0026rsquo;t access it via JavaScript now. If you still need to send the token with every request, it has become established to send it as a Bearer token, e.g. Authorization : Bearer \u0026lt;JWS\u0026gt;.\nSince the password only has to be sent with the first request and thus has to be checked, the password hash method used can now be set so that it requires at least 200ms for hashing. This also increases the password security.\nAs a further extra, it is now possible to determine whether a client has access in the reverse proxy or load balancer by checking the token there. For this there are free modules or has NGINX Plus from R10 native support for JWTs. On the one hand, this relieves the backend and on the other hand, components that know nothing about authentication, e.g. a CDN, can also be protected.\nWhat should you pay attention to?  These tokens should only be used for authentication and should not be abused as session cookies. JWT claims as payload of JWS elements are readable for everyone, thus only such information should be stored/transferred there that may also be visible for everyone. JWS elements without a signature should be considered invalid. The exp claim should always be used to restrict the validity of a token. The jti claim should always be used to prevent replay attacks. When using JWTs to authenticate REST APIs, the JWT should be stored in a cookie with HttpOnly set. If frontend and REST backend are accessible through the same domain, the cookie should use SameSite, even if it is supported only by Chromium/ Chrome and Opera so far.  WeiterfÃ¼hrende Links  RFC 7515: JSON Web Signature (JWS) RFC 7516: JSON Web Encryption (JWE) RFC 7517: JSON Web Key (JWK) RFC 7518: JSON Web Algorithms (JWA) RFC 7519: JSON Web Token (JWT) RFC 7520: Examples of Protecting Content Using JSON Object Signing and Encryption (JOSE) jwt.io: Website about JWTs by auth0 with a free, detailed manual about JWTs. ","ref":"/blog/jwts-and-basic-auth/"},{"title":"Nexus as a binary repository for APKs","date":"","description":"Brief tutorial, to show how to use Nexus OSS as a binary repository for APKs","body":"If you use Nexus OSS to store your finished binary artifacts, you will of course want to do the same for your APKs. Since I\u0026rsquo;m currently working on an Android app professionally, I\u0026rsquo;ve now had the chance to gain my experience with it. Because I don\u0026rsquo;t want to make the same pitfalls again next time, I will explain it here step by step.\nMy previous experiences with Gradle are (fortunately) hardly worth mentioning, so first an extensive Google research was on the agenda. This soon led me to the Maven plugin for Gradle and the following configuration for uploadArchives:\napply plugin: \u0026#39;maven\u0026#39; uploadArchives { repositories { mavenDeployer { repository(url: \u0026#34;file://localhost/tmp/myRepo/\u0026#34;) } } } A corresponding call with gradlew clean assemble uploadArchives ran successfully, but no artifacts were stored. So I then became aware of the Gradle user guide Publishing artifacts and realized that the APKs must be declared so that uploadArchives also knows what is to be uploaded.\nTo add the APKs of all build variants (debug, release and/ or productflavors) created with assemble the following addition helps:\nandroid.applicationVariants.all { variant -\u0026gt; def apkFile = file(variant.outputs.outputFile[0]) tasks.\u0026#34;assemble${variant.name.capitalize()}\u0026#34; \u0026lt;\u0026lt; { artifacts { archives file: apkFile } } } Unfortunately, now with the added APKs uploadArchives does not work anymore, because within one POM there must not be multiple artifacts of the same type and classifier. The following error message appears:\n:app:uploadArchives FAILED FAILURE: Build failed with an exception. * What went wrong: Execution failed for task \u0026#39;:app:uploadArchives\u0026#39;. \u0026gt; Could not publish configuration \u0026#39;archives\u0026#39; \u0026gt; A POM cannot have multiple artifacts with the same type and classifier. Already have MavenArtifact app:apk:apk:null, trying to add MavenArtifact app:apk:apk:null. So the configuration of the artifacts has to be extended a bit. The easiest way to do this is via the classifier, as in the following snippet:\nandroid.applicationVariants.all { variant -\u0026gt; def apkFile = file(variant.outputs.outputFile[0]) tasks.\u0026#34;assemble${variant.name.capitalize()}\u0026#34; \u0026lt;\u0026lt; { artifacts { archives file: apkFile, name: variant.baseName, type: \u0026#39;apk\u0026#39;, classifier: variant.baseName } } } This completes the first part. The APKs end up in the configured repository. Now only the default values of the POM generation have to be adjusted if needed, in my case groupId, artifactId and version:\napply plugin: \u0026#39;maven\u0026#39; uploadArchives { repositories { mavenDeployer { repository(url: \u0026#34;file://localhost/tmp/myRepo/\u0026#34;) pom { groupId \u0026#39;de.sainth.android\u0026#39; artifactId \u0026#39;meineTolleApp\u0026#39; version android.defaultConfig.versionName } } } } Last but not least, replace the local repository with the real one. To avoid having to check in the credentials for the Nexus, it is a good idea to store them in the central ~/.gradle/gradle.properties and point to it.\nnexusUser=Benutzername nexusPassword=Passwort This is how the following configuration for the interaction of Gradle and Nexus for APK storage is created in the end:\napply plugin: \u0026#39;maven\u0026#39; android.applicationVariants.all { variant -\u0026gt; def apkFile = file(variant.outputs.outputFile[0]) tasks.\u0026#34;assemble${variant.name.capitalize()}\u0026#34; \u0026lt;\u0026lt; { artifacts { archives file: apkFile, name: variant.baseName, type: \u0026#39;apk\u0026#39;, classifier: variant.baseName } } } uploadArchives { repositories { mavenDeployer { repository(url: \u0026#34;https://mein.nexus.de/repo/\u0026#34;) { authentication(userName: \u0026#34;$nexusUser\u0026#34;, password: \u0026#34;$nexusPassword\u0026#34;) } pom { groupId \u0026#39;de.sainth.android\u0026#39; artifactId \u0026#39;meineTolleApp\u0026#39; version android.defaultConfig.versionName } } } } ","ref":"/blog/use-nexus-as-a-binary-repository-for-apks/"},{"title":"Asciidoctor PDF","date":"","description":"The presentation of a tool for simple and beautiful documents","body":"In the meantime, a company I know has noticed that it has also encountered the problem described here. After all, any kid can use Word, right? As much as I would love to utter sentences like \u0026ldquo;I told you so.\u0026rdquo; in these moments, I also like to offer my help in such situations. For this reason, one of the documents that are regularly updated and versioned and made available to third parties will now be switched to AsciiDoc as a test. At the same time git will be introduced for change tracking and versioning, but that\u0026rsquo;s another challenge I might write about some other time.\nI must admit that I was also pleased about this task. So I finally had a reason to deal with AsciiDoc in more detail and could push the unfortunately already neglected pet projects even further back. I have to say that my first positive impression about AsciiDoc has only been strengthened. Since it is important for the company to have a PDF in the corporate design at the end, I was glad to have come across Asciidoctor PDF, which generates PDFs directly from AsciiDoc. I never liked the \u0026ldquo;detour\u0026rdquo; with DocBook as an intermediate format when generating the PDF, especially because I know how time-consuming such design adjustments can sometimes be with DocBook.\nAsciidoctor PDF is written entirely in Ruby and is also easily installable on Windows systems. All possible design customizations of Asciidoctor PDF are documented in the Theming-Guide and are then configured in a YAML file.\n!!! warning \u0026ldquo;Warning\u0026rdquo; To avoid problems with non-ASCII characters (e.g. umlauts) it is important that the command line used to invoke Asciidoctor PDF is encoded with UTF-8. This should be the case directly under Linux, but under Windows the command chcp 65001 must be executed beforehand for this.\nOne point that also gave me a headache for some time is the lack of configurability of the cover page. In many places, one stumbles across the most diverse requirements for a cover sheet, which cannot be fulfilled at first. But for every problem there is a solution and also in this case I found a solution that I like very much. In AsciiDoc you can use the option :front-cover-image: to include an image that is rendered directly behind the cover page. Combining this with the :notitle: option, which prevents the actual cover page from being rendered, results in a customized cover page. The \u0026ldquo;front cover image\u0026rdquo; is simply a PDF that I created with LibreOffice Writer.\nBut I also have to disagree with myself on one point. I touted AsciiDocFx as an editor at the time, but have since come to the decision that Visual Studio Code paired with the plugins from asciidoctor is the better editor.\n","ref":"/blog/asciidoctor-pdf/"},{"title":"A different thinking approach","date":"","description":"Functional programming requires a different way of thinking","body":"Unfortunately, some time has passed again since the last post. In the last post I wrote something about the history of functional and imperative programming. This one is now to deal with the, compared to imperative programming, different way of thinking. The source code examples shown here are based on Java for the imperative examples and Haskell for the functional examples. I will try to use only functions available in the language standard and not library functions.\nIn imperative programming, one describes in detail the steps that must be performed by the computer to accomplish the task. In contrast, functional programming does not describe HOW something is to be computed, but WHAT. Not for nothing functional programming languages belong to the declarative programming languages.\nBy the functions, just as in mathematics, the \u0026ldquo;world\u0026rdquo; is represented or defined in the actual state. In general, functional programming with its terminology and concepts is very strongly based on mathematics, which makes it not easy for not math affine humans. If one managed to understand the concepts the large advantage is that one can rely on the mathematical background of these concepts. Thus, the correctness of a program can then be proven with \u0026ldquo;relatively\u0026rdquo; little effort, at least compared to imperative programs.\nWhile most imperative algorithms are based on the manipulation of memory areas (called variables), the functional algorithms can be implemented by term substitution. To illustrate this difference, the following example of computation of the sum of 1 to n will server.\nLet\u0026rsquo;s look at the imperative example first:\nint computeSumTill(int n) { int result = 0; for (int counter = 1; counter \u0026lt;= n; counter++) { result += counter; } return result; } When executing the upper function, the variables result and counter are changed again and again during each loop pass. For n=3 it looks like this:\n-- Initialization result := 0 counter := 1 -- 1. Loop pass result := 1 counter := 2 -- 2. Loop pass result := 3 counter := 3 -- 3. Loop pass result := 6 counter := 4 By comparison, a function for this computation would look like this:\ncomputeSumTill :: Int -\u0026gt; Int computeSumTill 0 = 0 computeSumTill n = n + computeSumTill (n-1) When Haskell is executed, term substitution is now applied and only at the end something is computed:\ncomputeSumTill 3 -- Apply computeSumTill 3 + computeSumTill (3-1) -- Apply (-) 3 + computeSumTill 2 -- Apply computeSumTill 3 + 2 + computeSumTill (2-1) -- Apply (-) 3 + 2 + computeSumTill 1 -- Apply computeSumTill 3 + 2 + 1 + computeSumTill (1-1) -- Apply (-) 3 + 2 + 1 + computeSumTill 0 -- Apply computeSumTill 3 + 2 + 1 + 0 -- Apply (+) 5 + 1 + 0 -- Apply (+) 6 + 0 -- Apply (+) 6 You can already see the different approaches in this simple example. In the imperative example a loop is used and in the functional example recursion. Of course, recursion is also possible in imperative languages, but it is uncommon, since it naturally involves certain dangers that do not disappear even in Haskell. But that is a separate topic, which I hope to take up again sometime.\nBut one can also see the different way of interpretation. While in the imperative language the variables are changed again and again, with the functional language first the entire formula is built up by term substitution, in order to compute it afterwards. One could also say that the functional approach corresponds to the manual approach. Term substitution is in fact what we also do manually when solving an equation.\n","ref":"/blog/a-different-thinking-approach/"},{"title":"A brief history of functional programming","date":"","description":"A brief historical overview of the roots of functional programming","body":"Functional programming is on everyone\u0026rsquo;s lips and concepts from functional programming are being adopted everywhere in imperative languages. At the moment I am also taking a closer look at these concepts and would like to publish my findings here in the form of a series of articles. But let\u0026rsquo;s start with a short historical review.\nI have to admit that the more I researched the story, the more fascinating I found it.\nIt began in 1936, when Alonzo Church\u0026rsquo;s April publication \u0026ldquo;An unsolvable Problem of Elementary Number Theory\u0026rdquo; suggested that the decision problem was unsolvable.\n \u0026hellip;the Entscheidungsproblem is unsolvable in the case of any system of symbolic logic which is Ï‰-consistent (Ï‰-widerspruchsfrei) in the sense of GÃ¶del (loc. cit., p. 187) and is strong enough to allow certain comparatively simple methods of definition and proof.\n\u0026ndash; Alonzo Church, An unsolvable Problem of Elementary Number Theory\n To reach this conclusion, he defined the Lambda calculus, which laid the foundation for functional programming.\nAt about the same time, Alan Turing also dealt with the decision problem and published \u0026ldquo;On computable numbers, with an application to the Entscheidungsproblem\u0026rdquo; in August 1936. In it he had to refer to Church\u0026rsquo;s work, which probably displeased him somewhat (see Alan Turing â€” a short biography).\n In a recent paper Alonzo Church has introduced an idea of \u0026ldquo;effective calculability\u0026rdquo;, which is equivalent to my \u0026ldquo;computability\u0026rdquo;, but is very differently defined. Church also reaches similar conclusions about the Entscheidungsproblem.\n\u0026ndash; Alan Turing, On computable numbers, with an application to the Entscheidungsproblem\n Part of Alan Turing\u0026rsquo;s paper was the definition of automatic machines or a-machines, which are better known today as Turing machines and laid the foundation for imperative programming. In the appendix of said paper Turing also states that the classes \u0026ldquo;effective calculability\u0026rdquo; (lambda calculus) and \u0026ldquo;computability\u0026rdquo; (Turing machine) are equally powerful.\nTuring came to Princeton University in September 1936 and began his PhD under Church. I also find it remarkable that other well-known personalities in computer science received their PhDs under Church, such as Kleene, Scott, Kemeny\nAs you can see from these excerpts from history, the respective basic concepts of both programming paradigms (functional and imperative) emerged, interestingly enough, in 1936, but independently of each other as proof that the decision problem is unsolvable.\n","ref":"/blog/brief-history-of-functional-programming/"},{"title":"Word & Co have too many disadvantages","date":"","description":"An appeal to also give other programs than Word a chance","body":"I would like to dedicate this post to my little sister and all the others who think they have to write term papers, theses etc. with Word and are not able to integrate page numbers, tables of contents or other necessities. The bottom line is that such papers are really just about creating a PDF with the desired content to either submit digitally or print out.\nWhat\u0026rsquo;s wrong with WYSIWYG word processors?  They offer too many possibilities to use them incorrectly. It already starts with the fact that you can format a paragraph independently of its style sheet. If you are not careful enough it results in uncontrolled growth. I saw documents where the author was surprised that the generation of the table of contents did not work. The cause was quickly found when looking at the structure of the document. There were no headings for the program, because instead of the correct style sheet, the designated headings were formatted manually. So for the human eye, the headings were there, but all the supposed headings had the Paragraph style. Versioning or change tracking is complicated and many have never heard of it. In addition, it is cumbersome to work with several people on a single document, because the different versions of a document must first be merged back into one document.  What are the alternatives?  TeX respectively LaTeX: LaTeX is actually the standard for elaborations in the STEM environment. It has a very clean layout with excellent formula support. It offers almost as many formatting possibilities as Word \u0026amp; Co and thanks to pdflatex you can also create PDF files directly. There are many editors, tutorials and templates on the internet. I also wrote almost all my papers with LaTeX during my studies. I used Texmaker as editor and my template for my thesis was the general template of Matthias Pospiech, which offers a very good basis and is super documented. DocBook: DocBook is OASIS standard and based on XML. XML is not exactly known for its human readability, but after about practice you can get used to it. Unfortunately I don\u0026rsquo;t know of any free WYSIWYG editors for DocBook. There is a wide variety of output formats, HTML, EPUB, and PDF being probably the most interesting of them. If you don\u0026rsquo;t like the standard layout, you will need a good knowledge of XSL or XSLT. I had the task to make exactly such layout adjustments a few years ago and would say that such changes are only worth the effort if you want to use the corresponding stylesheets in the long run. AsciiDoc: AsciiDoc uses a simplified markup which is like Markdown with additional metadata. This also makes the source code human readable. This makes it easy to focus on the text when creating without being distracted by the necessary markup. AsciiDoc uses HTML and DocBook as output formats and can thus be processed to PDF via DocBook as an intermediate format, but is of course subject to the same requirements for an appropriate result. pandoc: Pandoc is supposed to be the Swiss army knife when you want to translate one text format into another. According to the Pandoc homepage there are 21 input formats and 37 output formats, whereby all formats presented so far are also represented as input formats. PDF is supported as output format via LaTeX as intermediate format.  There are certainly some other alternatives, but I will leave it at this point. Personally, I have already worked with LaTeX and DocBook and the other two alternatives presented use LaTeX and DocBook respectively as an intermediate format on the way to PDF. Both LaTeX, as well as DocBook, require a corresponding training period. Unless you need LaTeX\u0026rsquo;s powerful formula engine, or perhaps even need to use a particular LaTeX template and have no experience with it so far, I wouldn\u0026rsquo;t necessarily recommend LaTeX. DocBook I would only recommend to those who already have experience with HTML and accordingly the packaging of the texts into suitable tags. I can\u0026rsquo;t judge how good pandoc really is, but the more features an application has, the higher the probability that some part of it is buggy. Therefore, I am an advocate of the KISS principle and would lean towards AsciiDoc.\nAlthough it was also part of my research to look for possible editors for this article, but with the results I would now like to limit myself to my recommendation AsciiDoc. Very informative was the page of Asciidoctor, which provides a good overview. I would like to emphasize AsciiDocFX. AsciiDocFX is a standalone editor written with JavaFX and therefore requires an Oracle JRE to run. It offers an integrated live preview and also the PDF export option I wanted. I especially liked the possibility to create a Sample Book, which first shows all the basic functions for creating an elaboration in an example.\n","ref":"/blog/word-is-too-complex/"},{"title":"The Phoenix Project","date":"","description":"A brief book review of 'The Phoenix Project'","body":"I never thought I would actually read a book that is both a technical book and a novel, that would really excite me enough to want to write a review about it. But this hardly imaginable situation has now occurred. First of all, I would like to state that I was not promised any benefits for this post and that I purchased the book quite regularly in a store. It is the book The Phoenix Project - A Novel About IT, DevOps, and Helping Your Business Win by Gene Kim, Kevin Behr, and George Spafford.\nThe Phoenix Project inimitably tells the story of Bill Palmer, a midrange computer department manager at Parts Unlimited, an auto parts manufacturer. Parts Unlimited\u0026rsquo;s market situation is very tight, with competitors always one step ahead. Bill\u0026rsquo;s survival tactic has always been to fly under the radar and not stand out. So the day comes when Bill, running late for work, is called into her office by the HR manager on his way to work. Once there, he learns that his boss and his boss\u0026rsquo;s boss, the CIO, are no longer with the company and that he is now to become the head of IT operations and that this choice was made by the CEO himself. Bill firmly intends to refuse the promotion, but somehow the charismatic CEO Steve Masters manages to run him over in conversation so that he does not manage to refuse the promotion.\nDuring the conversation with Steve, he learns that there are IT problems with the payroll and that he should contact Dick, the CFO. Finding the cause is not easy, since there is also a failure in the SAN parallel to the problems with the payroll. This is how the story takes its course. At first, Bill tries to cope with the chaos through precision and discipline, as he learned in the Marines, and to put structures in place, as he was taught in business school.\nDuring the book, Bill meets Erik. Erik is supposed to join the board if the chairman of the board Bob Strauss and Steve Masters have their way. Erik doesn\u0026rsquo;t fit the profile of the classic businessman at all. He explains to Bill in worn pants and a T-shirt, using one of Parts Unlimited\u0026rsquo;s factories as an example, that classic production and IT (development and operations) are actually no different and that it is therefore very easy to derive corresponding procedures for IT from the optimizations and processes of classic production.\nThus, there are all kinds of problems that need to be solved in the book, and you learn about the necessary considerations and steps for DevOps on the basis of the three paths. The often difficult relationship between developers and administrators is also highlighted. You can imagine the situations/problems described really happening in reality. The book is also a wonderful reference work, as the essence of the book is also summarized in the appendix, which is easy to understand and detached from the actual story.\nIn conclusion, this book should really be a standard work, whether you are a developer, administrator, or especially if you are part of management and wonder why IT departments don\u0026rsquo;t tick the way you think they do.\n","ref":"/blog/the-phoenix-project/"}]